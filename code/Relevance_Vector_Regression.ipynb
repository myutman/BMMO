{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Relevance Vector Regression.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myutman/BMMO/blob/master/code/Relevance_Vector_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js6-6SJO81oU"
      },
      "source": [
        "# Лабораторная работа по Relevance Vector Regression\n",
        "В рамках этой лабораторной работы необходимо:\n",
        "- Имплементировать Relevance Vector Regression\n",
        "- Применить на синетическом датасете (восстановление полинома), сравнить с Lasso из sklearn и гребневой регрессией\n",
        "- Применить на данных sinc с RBF признаками, визуализировать \"релевантные вектора\", сравнить с Support Vector Regression и Lasso\n",
        "- Сделать выводы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq87isKE81oW"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from __future__ import print_function\n",
        "\n",
        "%matplotlib inline\n",
        "np.random.seed(123)\n",
        "\n",
        "def l2_error(X, t, w):\n",
        "    return np.sum((X.dot(w.ravel()) - t) ** 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opRPbIxL81od"
      },
      "source": [
        "## Имплементация Relevance Vector Regression\n",
        "\n",
        "Здесь необходимо реализовать три функции:\n",
        "\n",
        "1. `get_w_sigma(X, t, alpha, beta)`, которая принимает датасет (X, t) и гиперпараметры RVR (alpha, beta) и возвращает параметры апостериорного распределения mu, sigma\n",
        "2. `update_alpha_beta(X, t, alpha, beta)`, которая принимает датасет (X, t) и гиперпараметры RVR (alpha, beta) и делает один шаг итерационной процедуры для обновления гиперпараметров (было на лекции)\n",
        "3. `fit_rvr(X, t, max_iters)`, которая принимает датасет (X, t) и максимальное количество итераций и возвращает обученные гиперпараметры и параметры апостериорного распределения на веса модели\n",
        "\n",
        "На что стоит обратить внимание:\n",
        "\n",
        "1. Результаты дорогостоящих операций типа перемножения одних и тех же матриц нужно кешировать и переиспользовать\n",
        "2. $\\alpha$-ы для нерелевантных объектов должны принять значение `np.inf`, а соответствующие веса и их дисперсии должны иметь значение 0\n",
        "3. Бесконечности и нули из предыдущего пункта должны обрабатываться корректно, без NaN-ов и warning-ов\n",
        "4. Матрицу с бесконечными элементами на диагонали можно обращать более эффективно (достаточно обратить подматрицу)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-0nMN2b81oe"
      },
      "source": [
        "def get_w_sigma(X, t, alpha, beta):\n",
        "    \"\"\"Calculate the mean and the covariance matrix\n",
        "       of the posterior distribution\"\"\"\n",
        "    n, d = X.shape\n",
        "    \n",
        "    # YOUR CODE GOES HERE\n",
        "    \n",
        "    return w, sigma\n",
        "\n",
        "\n",
        "def update_alpha_beta(X, t, alpha, beta):\n",
        "    \"\"\"Update the hyperperemeters to increase evidence\"\"\"\n",
        "\n",
        "    # YOUR CODE GOES HERE\n",
        "    \n",
        "    return alpha_new, beta_new\n",
        "\n",
        "\n",
        "def fit_rvr(X, t, max_iter=10000):\n",
        "    \"\"\"Train the Relevance Vector Regression model\"\"\"\n",
        "\n",
        "    # YOUR CODE GOES HERE\n",
        "    \n",
        "    return w, sigma, alpha, beta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQuz6U-u81oi"
      },
      "source": [
        "## Восстановление полинома\n",
        "\n",
        "Здесь решается модельная задача: зашумленным полиномом третьей степени сгенерированы данные для задачи регрессии. Нужно на этих данных обучить многочлен степени, не превышающей 20. Предлагается сравнить три модели: гребневую регрессию, L1-регрессию (Lasso) и RVR, и сравнить ошибку на тестовой выборке и качество отобранных признаков."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "8RmmaqxD81oj"
      },
      "source": [
        "# Data generation\n",
        "\n",
        "def gen_batch(n, w, beta):\n",
        "    d = len(w)\n",
        "    X = np.random.uniform(-1, 1, (n, 1))\n",
        "    X = np.sort(X, axis=0)\n",
        "    X = np.hstack([X ** i for i in range(d)])\n",
        "    t = X.dot(w) + np.random.normal(size=n) / beta ** 0.5\n",
        "    return X, t\n",
        "\n",
        "n = 200\n",
        "d = 21\n",
        "w_true = np.zeros(d)\n",
        "w_true[1] = 1\n",
        "w_true[3] = -1\n",
        "beta_true = 100\n",
        "\n",
        "X_train, t_train = gen_batch(n, w_true, beta_true)\n",
        "X_test, t_test = gen_batch(n, w_true, beta_true)\n",
        "\n",
        "# Visualization\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X_train[:, 1], t_train, s=3, label='Train data', alpha=0.3)\n",
        "ax.scatter(X_test[:, 1], t_test, s=3, label='Test data', alpha=0.3)\n",
        "ax.plot(X_train[:, 1], X_train.dot(w_true), label='Ground truth')\n",
        "\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.legend(ncol=3, loc=9, bbox_to_anchor=(0.5, 1.15))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwD4zc0H81om"
      },
      "source": [
        "# Relevance Vector Regression\n",
        "w_rvr, sigma_rvr, alpha_rvr, beta_rvr = fit_rvr(X_train, t_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIiLt7s681op"
      },
      "source": [
        "# Ridge Regression with Cross-Validation\n",
        "from sklearn.linear_model import RidgeCV\n",
        "ridge = RidgeCV(cv=20, alphas=10.**np.linspace(-6, 3, 100),\n",
        "                fit_intercept=False).fit(X_train, t_train)\n",
        "w_ridge = ridge.coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VG0pIjLe81or"
      },
      "source": [
        "# Lasso Regression with Cross-Validation\n",
        "from sklearn.linear_model import LassoCV, Lasso\n",
        "lasso = LassoCV(cv=5, alphas=10.**np.linspace(-6, 3, 100),\n",
        "                fit_intercept=False, max_iter=2000000).fit(X_train, t_train)\n",
        "w_lasso = lasso.coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "NJ7TuV3081os"
      },
      "source": [
        "# Comparison\n",
        "print('Relevance Vector Regression')\n",
        "print('Features remaining:', np.sum(alpha_rvr < 1e8), '/', d)\n",
        "print('Train error:', l2_error(X_train, t_train, w_rvr) / n)\n",
        "print('Test error: ', l2_error(X_test, t_test, w_rvr) / n)\n",
        "print('-'*50)\n",
        "print('Ridge Regression')\n",
        "print('Features remaining: NA (no sparsity)')\n",
        "print('Train error:', l2_error(X_train, t_train, w_ridge) / n)\n",
        "print('Test error: ', l2_error(X_test, t_test, w_ridge) / n)\n",
        "print('-'*50)\n",
        "print('Lasso Regression')\n",
        "print('Features remaining:', np.sum(np.abs(w_lasso) > 1e-20), '/', d)\n",
        "print('Train error:', l2_error(X_train, t_train, w_lasso) / n)\n",
        "print('Test error: ', l2_error(X_test, t_test, w_lasso) / n)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X_train[:, 1], t_train, s=3, label='Train data', alpha=0.3)\n",
        "ax.scatter(X_test[:, 1], t_test, s=3, label='Test data', alpha=0.3)\n",
        "ax.plot(X_train[:, 1], X_train.dot(w_true), label='Ground truth')\n",
        "ax.plot(X_train[:, 1], X_train.dot(w_rvr), label='RVR')\n",
        "ax.plot(X_train[:, 1], X_train.dot(w_ridge), label='Ridge')\n",
        "ax.plot(X_train[:, 1], X_train.dot(w_lasso), label='Lasso')\n",
        "\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.legend(ncol=3, loc=9, bbox_to_anchor=(0.5, 1.25))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG2LSSIh81ou"
      },
      "source": [
        "## Регрессия с RBF-признаками\n",
        "\n",
        "Здесь решается другая модельная задача: необходимо восстановить зашумленную функцию `sinc(x)`. Предлагается применить kernel trick с RBF-ядром (можно использовать функцию `sklearn.metrics.pairwise.rbf_kernel`), обучить три модели: SVM-регрессию (SVR), L1-регрессию (Lasso) и RVR, и сравнить ошибку на тестовой выборке и качество отобранных опорных / релевантных объектов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IVNWccn81ou"
      },
      "source": [
        "# Data generation\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "\n",
        "def gen_batch(n, beta):\n",
        "    points = np.random.uniform(-5, 5, n)\n",
        "    points = np.sort(points)\n",
        "    t = np.sinc(points) + np.random.normal(size=n) / beta ** 0.5\n",
        "    return points, t\n",
        "\n",
        "n = 200\n",
        "n_test = 1000\n",
        "d = n + 1\n",
        "beta_true = 100\n",
        "\n",
        "points_train, t_train = gen_batch(n, beta_true)\n",
        "points_test, t_test = gen_batch(n_test, beta_true)\n",
        "\n",
        "# RBF-transform\n",
        "X_train = # YOUR CODE GOES HERE\n",
        "X_test = # YOUR CODE GOES HERE\n",
        "\n",
        "# Constant feature\n",
        "X_train = np.hstack((np.ones((n, 1)), X_train))\n",
        "X_test = np.hstack((np.ones((n_test, 1)), X_test))\n",
        "\n",
        "# Visualization\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(points_train, t_train, s=3, label='Train data', alpha=1)\n",
        "ax.scatter(points_test, t_test, s=3, label='Test data', alpha=0.2)\n",
        "ax.plot(points_train, np.sinc(points_train), label='Ground truth')\n",
        "\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.legend(ncol=3, loc=9, bbox_to_anchor=(0.5, 1.15))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKbWcIdf81ow"
      },
      "source": [
        "# Relevance Vector Regression\n",
        "w_rvr, sigma_rvr, alpha_rvr, beta_rvr = fit_rvr(X_train, t_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjExVoaj81ox"
      },
      "source": [
        "# Lasso Regression with Cross-Validation\n",
        "from sklearn.linear_model import LassoCV\n",
        "lasso = LassoCV(cv=10, alphas=10.**np.linspace(-5, 1, 20),\n",
        "                fit_intercept=False, max_iter=100000, tol=1e-2, n_jobs=10).fit(X_train, t_train)\n",
        "w_lasso = lasso.coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oJMIREC81oz"
      },
      "source": [
        "# Support Vector Regression\n",
        "from sklearn.svm import SVR\n",
        "svr = SVR(gamma=1, tol=1e-6, C=1).fit(points_train.reshape(-1, 1), t_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q14Q7YJK81o0"
      },
      "source": [
        "# Comparison\n",
        "print('Relevance Vector Regression')\n",
        "print('Objects remaining:', np.sum(alpha_rvr[1:] < 1e8), '/', n)\n",
        "print('Train error:', l2_error(X_train, t_train, w_rvr) / n)\n",
        "print('Test error: ', l2_error(X_test, t_test, w_rvr) / n)\n",
        "print('-'*50)\n",
        "print('Lasso Regression')\n",
        "print('Objects remaining:', np.sum(np.abs(w_lasso[1:]) > 1e-20), '/', n)\n",
        "print('Train error:', l2_error(X_train, t_train, w_lasso) / n)\n",
        "print('Test error: ', l2_error(X_test, t_test, w_lasso) / n)\n",
        "print('-'*50)\n",
        "print('Support Vector Regression')\n",
        "print('Objects remaining:', len(svr.support_), '/', n)\n",
        "print('Train error:', np.sum((svr.predict(points_train.reshape(-1, 1)) - t_train) ** 2) / n)\n",
        "print('Test error: ', np.sum((svr.predict(points_test.reshape(-1, 1)) - t_test) ** 2) / n)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(points_train, t_train, s=3, label='Train data', alpha=0.3)\n",
        "ax.scatter(points_test, t_test, s=3, label='Test data', alpha=0.3)\n",
        "ax.plot(points_test, np.sinc(points_test), label='Ground truth')\n",
        "ax.plot(points_test, X_test.dot(w_rvr), label='RVR')\n",
        "ax.plot(points_test, X_test.dot(w_lasso), label='Lasso')\n",
        "ax.plot(points_test, svr.predict(points_test.reshape(-1, 1)), label='SVR')\n",
        "\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.legend(ncol=3, loc=9, bbox_to_anchor=(0.5, 1.25))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejoyGlGC81o2"
      },
      "source": [
        "### Визуализация релевантных объектов для RVR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MwbIiMY81o2"
      },
      "source": [
        "relevant = alpha_rvr[1:] < 1e8\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(points_train, t_train, s=3, label='Train data', alpha=0.3)\n",
        "ax.scatter(points_train[relevant], t_train[relevant], c='tab:blue', s=30, label='Relevant objects')\n",
        "ax.scatter(points_test, t_test, s=3, label='Test data', alpha=0.3)\n",
        "ax.plot(points_test, np.sinc(points_test), label='Ground truth')\n",
        "ax.plot(points_test, X_test.dot(w_rvr), label='RVR')\n",
        "\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.legend(ncol=3, loc=9, bbox_to_anchor=(0.5, 1.25))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6ZK5bLr81o3"
      },
      "source": [
        "## Выводы\n",
        "В этом поле опишите свои наблюдения и сформулируйте свои выводы"
      ]
    }
  ]
}